{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4accc8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import math\n",
    "from matplotlib import pyplot\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "from keras import optimizers, losses, activations, models\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from keras.layers import Dense, Input, Dropout, Convolution1D, MaxPool1D, GlobalMaxPool1D, GlobalAveragePooling1D, Concatenate\n",
    "from sklearn.metrics import f1_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5838d572",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"C:/00ETH/ml4h/Project1/archive/mitbih_train.csv\", header=None)\n",
    "df_train = df_train.sample(frac=1)\n",
    "df_test = pd.read_csv(\"C:/00ETH/ml4h/Project1/archive/mitbih_test.csv\", header=None)\n",
    "\n",
    "Y = np.array(df_train[187].values).astype(np.int8)\n",
    "X = np.array(df_train[list(range(187))].values)[..., np.newaxis]\n",
    "\n",
    "Y_test = np.array(df_test[187].values).astype(np.int8)\n",
    "X_test = np.array(df_test[list(range(187))].values)[..., np.newaxis]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f11cf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation 1 -> smoothing original time series\n",
    "\n",
    "def smooth_data(data, window_size):\n",
    "    rolling_transform = data.rolling(window=window_size)\n",
    "    rolling_mean = rolling_transform.mean()\n",
    "    return rolling_mean    \n",
    "\n",
    "X_smooth_small = smooth_data(df_train, 3)\n",
    "X_smooth_medium = smooth_data(df_train, 9)\n",
    "X_smooth_large = smooth_data(df_train, 15)\n",
    "\n",
    "X_test_smooth_small = smooth_data(df_test, 3)\n",
    "X_test_smooth_medium = smooth_data(df_test, 9)\n",
    "X_test_smooth_large = smooth_data(df_test, 15)\n",
    "\n",
    "X_smooth_small = X_smooth_small.sample(frac=1)\n",
    "X_smooth_medium = X_smooth_medium.sample(frac=1)\n",
    "X_smooth_large = X_smooth_large.sample(frac=1)\n",
    "\n",
    "X_test_smooth_small = X_test_smooth_small.sample(frac=1)\n",
    "X_test_smooth_medium = X_test_smooth_medium.sample(frac=1)\n",
    "X_test_smooth_large = X_test_smooth_large.sample(frac=1)\n",
    "\n",
    "X_smooth_small = np.array(X_smooth_small[list(range(187))].values)[..., np.newaxis]\n",
    "X_smooth_medium = np.array(X_smooth_medium[list(range(187))].values)[..., np.newaxis]\n",
    "X_smooth_large = np.array(X_smooth_large[list(range(187))].values)[..., np.newaxis]\n",
    "\n",
    "X_test_smooth_small = np.array(X_test_smooth_small[list(range(187))].values)[..., np.newaxis]\n",
    "X_test_smooth_medium = np.array(X_test_smooth_medium[list(range(187))].values)[..., np.newaxis]\n",
    "X_test_smooth_large = np.array(X_test_smooth_large[list(range(187))].values)[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "57a03a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation 2 -> downsample original time series\n",
    "def downsample_data(data, factor, sample_rate):\n",
    "    start = date(2000, 1, 1)\n",
    "    end = start + dt.timedelta(days=data.shape[1]-1)\n",
    "    index = pd.date_range(start, end)\n",
    "    s = pd.Series(data[0,:,0], index=index)\n",
    "    s = s.resample(sample_rate)\n",
    "    sample_mean = s.mean()\n",
    "    data_return = np.zeros([data.shape[0], sample_mean.shape[0]])\n",
    "    for i in range(1,data.shape[0]-1):\n",
    "        s = pd.Series(data[i,:,0], index=index)\n",
    "        s = s.resample(sample_rate)\n",
    "        sample_mean = s.mean()\n",
    "        sample_mean = sample_mean.to_numpy()\n",
    "        data_return[i,:] = sample_mean\n",
    "    return data_return\n",
    "\n",
    "X_sample_small = downsample_data(X, 1.2, '1.2D')\n",
    "X_sample_medium = downsample_data(X, 1.4, '1.4D')\n",
    "X_sample_large = downsample_data(X, 1.6, '1.6D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2974666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sample_small = downsample_data(X_test, 1.2, '1.2D')\n",
    "X_test_sample_medium = downsample_data(X_test, 1.4, '1.4D')\n",
    "X_test_sample_large = downsample_data(X_test, 1.6, '1.6D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a18e5e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample_small = X_sample_small[..., np.newaxis]\n",
    "X_sample_medium = X_sample_medium[..., np.newaxis]\n",
    "X_sample_large = X_sample_large[..., np.newaxis]\n",
    "\n",
    "X_test_sample_small = X_test_sample_small[..., np.newaxis]\n",
    "X_test_sample_medium = X_test_sample_medium[..., np.newaxis]\n",
    "X_test_sample_large = X_test_sample_large[..., np.newaxis]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "30e0a531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87554, 187, 1)\n",
      "(87554, 187, 1)\n",
      "(87554, 156, 1)\n",
      "(87554, 133, 1)\n",
      "(87554, 117, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X_smooth_small.shape)\n",
    "print(X_sample_small.shape)\n",
    "print(X_sample_medium.shape)\n",
    "print(X_sample_large.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dab87800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_model(input_len, filter_size):\n",
    "        inp = Input(shape=(input_len, 1))\n",
    "        img_1 = Convolution1D(int(filter_size/2), kernel_size=4, activation=activations.relu, padding=\"valid\")(inp)\n",
    "        img_1 = Convolution1D(int(filter_size/2), kernel_size=4, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "        img_1 = MaxPool1D(pool_size=2)(img_1)\n",
    "        img_1 = Dropout(rate=0.1)(img_1)\n",
    "        img_1 = Convolution1D(filter_size, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "        img_1 = Convolution1D(filter_size, kernel_size=3, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "        img_1 = MaxPool1D(pool_size=2)(img_1)\n",
    "        img_1 = Dropout(rate=0.1)(img_1)\n",
    "        img_1 = Convolution1D(filter_size, kernel_size=2, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "        img_1 = Convolution1D(filter_size, kernel_size=2, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "        img_1 = MaxPool1D(pool_size=2)(img_1)\n",
    "        img_1 = Dropout(rate=0.1)(img_1)\n",
    "        \n",
    "        dense = Dense(filter_size*2,activation=activations.relu, name=\"dense_1\")(img_1)\n",
    "        dense = Dropout(0.3)(dense)\n",
    "        model = models.Model(inputs = inp, outputs = dense)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bd32a79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_model2(input_len, filter_size):\n",
    "        inp = Input(shape=(input_len, 1))\n",
    "        img_1 = Convolution1D(int(filter_size/2), kernel_size=4, activation=activations.relu, padding=\"valid\")(inp)\n",
    "        img_1 = Convolution1D(int(filter_size/2), kernel_size=4, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "        img_1 = MaxPool1D(pool_size=2)(img_1)\n",
    "        img_1 = Dropout(rate=0.1)(img_1)\n",
    "        img_1 = Convolution1D(filter_size, kernel_size=2, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "        img_1 = Convolution1D(filter_size, kernel_size=2, activation=activations.relu, padding=\"valid\")(img_1)\n",
    "        img_1 = MaxPool1D(pool_size=2)(img_1)\n",
    "        img_1 = Dropout(rate=0.1)(img_1)\n",
    "        \n",
    "        dense = Dense(filter_size*2,activation=activations.relu, name=\"dense_1\")(img_1)\n",
    "        dense = Dropout(0.3)(dense)\n",
    "        model = models.Model(inputs = inp, outputs = dense)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "00d28ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model1(inputs_len = [187, 156 ,133 , 117], filter_sizes = [32, 16, 8]):\n",
    "    nclass = 5\n",
    "    \n",
    "    inp_smallseq = Input(shape=(inputs_len[1], 1))\n",
    "    inp_mediumseq = Input(shape=(inputs_len[2], 1))\n",
    "    inp_largeseq = Input(shape=(inputs_len[3], 1))\n",
    "    \n",
    "    inp_smooth1 = Input(shape=(inputs_len[0], 1))   \n",
    "    inp_smooth2 = Input(shape=(inputs_len[0], 1))   \n",
    "    inp_smooth3 = Input(shape=(inputs_len[0], 1))   \n",
    "    inp_org = Input(shape=(inputs_len[0], 1))\n",
    "    \n",
    "    base_net_org = get_base_model(inputs_len[0], filter_sizes[0])\n",
    "    \n",
    "    #smoothing\n",
    "    base_net1 = get_base_model(inputs_len[0], filter_sizes[0])\n",
    "    base_net2 = get_base_model(inputs_len[0], filter_sizes[0])\n",
    "    base_net3 = get_base_model(inputs_len[0], filter_sizes[0])\n",
    "    \n",
    "    #sampling\n",
    "    base_net_small = get_base_model(inputs_len[1], filter_sizes[0])\n",
    "    base_net_medium = get_base_model(inputs_len[2], filter_sizes[0])\n",
    "    base_net_large = get_base_model2(inputs_len[3], filter_sizes[0])\n",
    "    \n",
    "    embedding_org = base_net_org(inp_org)\n",
    "    \n",
    "    #smoothing\n",
    "    embedding_smooth1 = base_net1(inp_smooth1)\n",
    "    embedding_smooth2 = base_net2(inp_smooth2)\n",
    "    embedding_smooth3 = base_net3(inp_smooth3)\n",
    "    \n",
    "    #sampling\n",
    "    embedding_small = base_net_small(inp_smallseq)\n",
    "    embedding_medium = base_net_medium(inp_mediumseq)\n",
    "    embedding_large = base_net_large(inp_largeseq)\n",
    "    \n",
    "    # merge all the outputs\n",
    "    merged1 = Concatenate(axis = 1)([embedding_org,embedding_smooth1, embedding_smooth2, embedding_smooth3, embedding_small, embedding_medium, embedding_large ]) \n",
    "\n",
    "    opt = optimizers.Adam(0.001) \n",
    "    \n",
    "    merged1 = Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\")(merged1)\n",
    "    merged1 = GlobalMaxPool1D()(merged1)\n",
    "    merged1 = Dropout(rate=0.2)(merged1)\n",
    "    out1 = Dense(64, activation=activations.relu, name=\"dense_3_mitbih\")(merged1)\n",
    "    out1 = Dense(nclass, activation=activations.softmax, name=\"dense_4_mitbih\")(out1)   \n",
    "\n",
    "    model1 = models.Model(inputs=[inp_org,inp_smooth1, inp_smooth2, inp_smooth3, inp_smallseq, inp_mediumseq, inp_largeseq ], outputs=out1)\n",
    "    model1.compile(optimizer=opt, loss=losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
    "    model1.summary()\n",
    "\n",
    "    return model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "73aa59cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_289 (InputLayer)          (None, 187, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_286 (InputLayer)          (None, 187, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_287 (InputLayer)          (None, 187, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_288 (InputLayer)          (None, 187, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_283 (InputLayer)          (None, 156, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_284 (InputLayer)          (None, 133, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_285 (InputLayer)          (None, 117, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_134 (Model)               (None, 20, 64)       12064       input_289[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "model_135 (Model)               (None, 20, 64)       12064       input_286[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "model_136 (Model)               (None, 20, 64)       12064       input_287[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "model_137 (Model)               (None, 20, 64)       12064       input_288[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "model_138 (Model)               (None, 16, 64)       12064       input_283[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "model_139 (Model)               (None, 13, 64)       12064       input_284[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "model_140 (Model)               (None, 26, 64)       6368        input_285[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 135, 64)      0           model_134[1][0]                  \n",
      "                                                                 model_135[1][0]                  \n",
      "                                                                 model_136[1][0]                  \n",
      "                                                                 model_137[1][0]                  \n",
      "                                                                 model_138[1][0]                  \n",
      "                                                                 model_139[1][0]                  \n",
      "                                                                 model_140[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_821 (Conv1D)             (None, 133, 256)     49408       concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 256)          0           conv1d_821[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_537 (Dropout)           (None, 256)          0           global_max_pooling1d_16[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3_mitbih (Dense)          (None, 64)           16448       dropout_537[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_4_mitbih (Dense)          (None, 5)            325         dense_3_mitbih[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 144,933\n",
      "Trainable params: 144,933\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = get_model1(inputs_len = [187, 156 ,133 , 117], filter_sizes = [32, 16, 2])\n",
    "\n",
    "file_path = \"parallel_cnn1_mitbih.h5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5, verbose=1)\n",
    "redonplat = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", patience=3, verbose=2)\n",
    "callbacks_list = [checkpoint, early, redonplat]  # early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7eae8bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 78798 samples, validate on 8756 samples\n",
      "Epoch 1/1000\n",
      " - 156s - loss: nan - acc: 0.8275 - val_loss: nan - val_acc: 0.8296\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.82960\n",
      "Epoch 2/1000\n",
      " - 159s - loss: nan - acc: 0.8275 - val_loss: nan - val_acc: 0.8296\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.82960\n",
      "Epoch 3/1000\n",
      " - 160s - loss: nan - acc: 0.8275 - val_loss: nan - val_acc: 0.8296\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.82960\n",
      "Epoch 4/1000\n",
      " - 160s - loss: nan - acc: 0.8275 - val_loss: nan - val_acc: 0.8296\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.82960\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 5/1000\n",
      " - 160s - loss: nan - acc: 0.8275 - val_loss: nan - val_acc: 0.8296\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.82960\n",
      "Epoch 6/1000\n",
      " - 160s - loss: nan - acc: 0.8275 - val_loss: nan - val_acc: 0.8296\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.82960\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "X_train = [X, X_smooth_small, X_smooth_medium, X_smooth_large, X_sample_small, X_sample_medium, X_sample_large]\n",
    "X_test_array = [X_test, X_test_smooth_small, X_test_smooth_medium, X_test_smooth_large, X_test_sample_small, X_test_sample_medium, X_test_sample_large]\n",
    "\n",
    "\n",
    "#model.fit([X, X_smooth_small, X_smooth_medium, X_smooth_large], Y, epochs=1000, verbose=2, callbacks=callbacks_list, validation_split=0.1)\n",
    "model1.fit(X_train, Y, epochs=1000, verbose=2, callbacks=callbacks_list, validation_split=0.1)\n",
    "model1.load_weights(file_path)\n",
    "\n",
    "pred_test = model1.predict(X_test_array)\n",
    "pred_test = np.argmax(pred_test, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1c4b9ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test f1 score : 0.18113471632091976 \n",
      "Test accuracy score : 0.8276082587246483 \n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(Y_test, pred_test, average=\"macro\")\n",
    "\n",
    "print(\"Test f1 score : %s \"% f1)\n",
    "\n",
    "acc = accuracy_score(Y_test, pred_test)\n",
    "\n",
    "print(\"Test accuracy score : %s \"% acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bcf3306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model2(inputs_len = [187, 156 ,133 , 117], filter_sizes = [32, 16, 8]):\n",
    "    nclass = 5\n",
    "    \n",
    "    inp_smooth1 = Input(shape=(inputs_len[0], 1))   \n",
    "    inp_smooth2 = Input(shape=(inputs_len[0], 1))   \n",
    "    inp_smooth3 = Input(shape=(inputs_len[0], 1))   \n",
    "    inp_org = Input(shape=(inputs_len[0], 1))\n",
    "    \n",
    "    base_net_org = get_base_model(inputs_len[0], filter_sizes[0])\n",
    "    \n",
    "    #smoothing\n",
    "    base_net1 = get_base_model(inputs_len[0], filter_sizes[0])\n",
    "    base_net2 = get_base_model(inputs_len[0], filter_sizes[0])\n",
    "    base_net3 = get_base_model(inputs_len[0], filter_sizes[0])\n",
    "    \n",
    "    embedding_org = base_net_org(inp_org)\n",
    "    \n",
    "    #smoothing\n",
    "    embedding_smooth1 = base_net1(inp_smooth1)\n",
    "    embedding_smooth2 = base_net2(inp_smooth2)\n",
    "    embedding_smooth3 = base_net3(inp_smooth3)\n",
    "    \n",
    "    # merge all the outputs\n",
    "    merged2 = Concatenate(axis = 1)([embedding_org,embedding_smooth1, embedding_smooth2, embedding_smooth3]) \n",
    "\n",
    "    opt = optimizers.Adam(0.001) \n",
    "    \n",
    "    merged2= Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\")(merged2)\n",
    "    merged2 = GlobalMaxPool1D()(merged2)\n",
    "    merged2 = Dropout(rate=0.2)(merged2)\n",
    "    out2 = Dense(64, activation=activations.relu, name=\"dense_3_mitbih\")(merged2)\n",
    "    out2 = Dense(nclass, activation=activations.softmax, name=\"dense_4_mitbih\")(out2)   \n",
    "\n",
    "    model2 = models.Model(inputs=[inp_org,inp_smooth1, inp_smooth2, inp_smooth3], outputs=out2)\n",
    "    model2.compile(optimizer=opt, loss=losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
    "    model2.summary()\n",
    "    \n",
    "    return model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ee35b20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_300 (InputLayer)          (None, 187, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_297 (InputLayer)          (None, 187, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_298 (InputLayer)          (None, 187, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_299 (InputLayer)          (None, 187, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_142 (Model)               (None, 20, 64)       12064       input_300[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "model_143 (Model)               (None, 20, 64)       12064       input_297[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "model_144 (Model)               (None, 20, 64)       12064       input_298[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "model_145 (Model)               (None, 20, 64)       12064       input_299[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 80, 64)       0           model_142[1][0]                  \n",
      "                                                                 model_143[1][0]                  \n",
      "                                                                 model_144[1][0]                  \n",
      "                                                                 model_145[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_846 (Conv1D)             (None, 78, 256)      49408       concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_17 (Global (None, 256)          0           conv1d_846[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_554 (Dropout)           (None, 256)          0           global_max_pooling1d_17[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3_mitbih (Dense)          (None, 64)           16448       dropout_554[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_4_mitbih (Dense)          (None, 5)            325         dense_3_mitbih[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 114,437\n",
      "Trainable params: 114,437\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = get_model2(inputs_len = [187, 156 ,133 , 117], filter_sizes = [32, 16, 2])\n",
    "\n",
    "file_path = \"parallel_cnn2_mitbih.h5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5, verbose=1)\n",
    "redonplat = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", patience=3, verbose=2)\n",
    "callbacks_list = [checkpoint, early, redonplat]  # early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7f744371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 78798 samples, validate on 8756 samples\n",
      "Epoch 1/1000\n",
      " - 125s - loss: nan - acc: 0.8272 - val_loss: nan - val_acc: 0.8296\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.82960, saving model to parallel_cnn2_mitbih.h5\n",
      "Epoch 2/1000\n",
      " - 121s - loss: nan - acc: 0.8275 - val_loss: nan - val_acc: 0.8296\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.82960\n",
      "Epoch 3/1000\n",
      " - 121s - loss: nan - acc: 0.8275 - val_loss: nan - val_acc: 0.8296\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.82960\n",
      "Epoch 4/1000\n",
      " - 120s - loss: nan - acc: 0.8275 - val_loss: nan - val_acc: 0.8296\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.82960\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 5/1000\n",
      " - 128s - loss: nan - acc: 0.8275 - val_loss: nan - val_acc: 0.8296\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.82960\n",
      "Epoch 6/1000\n",
      " - 128s - loss: nan - acc: 0.8275 - val_loss: nan - val_acc: 0.8296\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.82960\n",
      "Epoch 00006: early stopping\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 4 array(s), but instead got the following list of 1 arrays: [array([[[1.        ],\n        [0.75826448],\n        [0.11157025],\n        ...,\n        [0.        ],\n        [0.        ],\n        [0.        ]],\n\n       [[0.90842491],\n        [0.7838828 ],\n        ...",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14540\\3225694524.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mpred_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mpred_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\envs\\ml4h\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1147\u001b[0m                              'argument.')\n\u001b[0;32m   1148\u001b[0m         \u001b[1;31m# Validate user data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1149\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1150\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\envs\\ml4h\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\envs\\ml4h\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[1;34m'Expected to see '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' array(s), '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m                 \u001b[1;34m'but instead got the following list of '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             raise ValueError(\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 4 array(s), but instead got the following list of 1 arrays: [array([[[1.        ],\n        [0.75826448],\n        [0.11157025],\n        ...,\n        [0.        ],\n        [0.        ],\n        [0.        ]],\n\n       [[0.90842491],\n        [0.7838828 ],\n        ..."
     ]
    }
   ],
   "source": [
    "X_train = [X, X_smooth_small, X_smooth_medium, X_smooth_large]\n",
    "\n",
    "#model.fit([X, X_smooth_small, X_smooth_medium, X_smooth_large], Y, epochs=1000, verbose=2, callbacks=callbacks_list, validation_split=0.1)\n",
    "model2.fit(X_train, Y, epochs=1000, verbose=2, callbacks=callbacks_list, validation_split=0.1)\n",
    "model2.load_weights(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "32d1a3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_array = [X_test, X_test_smooth_small, X_test_smooth_medium, X_test_smooth_large]\n",
    "\n",
    "pred_test = model2.predict(X_test_array)\n",
    "pred_test = np.argmax(pred_test, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8e0a849f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test f1 score : 0.18113471632091976 \n",
      "Test accuracy score : 0.8276082587246483 \n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(Y_test, pred_test, average=\"macro\")\n",
    "\n",
    "print(\"Test f1 score : %s \"% f1)\n",
    "\n",
    "acc = accuracy_score(Y_test, pred_test)\n",
    "\n",
    "print(\"Test accuracy score : %s \"% acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5b323679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model3(inputs_len = [187, 156 ,133 , 117], filter_sizes = [ 32, 16, 8]):\n",
    "    nclass = 5\n",
    "    \n",
    "    inp_smallseq = Input(shape=(inputs_len[1], 1))\n",
    "    inp_mediumseq = Input(shape=(inputs_len[2], 1))\n",
    "    inp_largeseq = Input(shape=(inputs_len[3], 1))  \n",
    "    inp_org = Input(shape=(inputs_len[0], 1))\n",
    "    \n",
    "    base_net_org = get_base_model(inputs_len[0], filter_sizes[0])\n",
    "    \n",
    "    #sampling\n",
    "    base_net_small = get_base_model(inputs_len[1], filter_sizes[0])\n",
    "    base_net_medium = get_base_model(inputs_len[2], filter_sizes[0])\n",
    "    base_net_large = get_base_model2(inputs_len[3], filter_sizes[0])\n",
    "    \n",
    "    embedding_org = base_net_org(inp_org)\n",
    "    \n",
    "    #sampling\n",
    "    embedding_small = base_net_small(inp_smallseq)\n",
    "    embedding_medium = base_net_medium(inp_mediumseq)\n",
    "    embedding_large = base_net_large(inp_largeseq)\n",
    "    \n",
    "    # merge all the outputs\n",
    "    merged3 = Concatenate(axis = 1)([embedding_org,embedding_small, embedding_medium, embedding_large ]) \n",
    "    \n",
    "    opt = optimizers.Adam(0.001) \n",
    "       \n",
    "    merged3= Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\")(merged3)\n",
    "    merged3 = GlobalMaxPool1D()(merged3)\n",
    "    merged3 = Dropout(rate=0.2)(merged3)\n",
    "    out3 = Dense(64, activation=activations.relu, name=\"dense_3_mitbih\")(merged3)\n",
    "    out3 = Dense(nclass, activation=activations.softmax, name=\"dense_4_mitbih\")(out3)   \n",
    "\n",
    "    model3 = models.Model(inputs=[inp_org,inp_smallseq, inp_mediumseq, inp_largeseq], outputs=out3)\n",
    "    model3.compile(optimizer=opt, loss=losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
    "    model3.summary()\n",
    "   \n",
    "    return model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2b447dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_308 (InputLayer)          (None, 187, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_305 (InputLayer)          (None, 156, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_306 (InputLayer)          (None, 133, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_307 (InputLayer)          (None, 117, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_147 (Model)               (None, 20, 64)       12064       input_308[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "model_148 (Model)               (None, 16, 64)       12064       input_305[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "model_149 (Model)               (None, 13, 64)       12064       input_306[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "model_150 (Model)               (None, 26, 64)       6368        input_307[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 75, 64)       0           model_147[1][0]                  \n",
      "                                                                 model_148[1][0]                  \n",
      "                                                                 model_149[1][0]                  \n",
      "                                                                 model_150[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_869 (Conv1D)             (None, 73, 256)      49408       concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 256)          0           conv1d_869[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_570 (Dropout)           (None, 256)          0           global_max_pooling1d_18[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3_mitbih (Dense)          (None, 64)           16448       dropout_570[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_4_mitbih (Dense)          (None, 5)            325         dense_3_mitbih[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 108,741\n",
      "Trainable params: 108,741\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3 = get_model3(inputs_len = [187, 156 ,133 , 117], filter_sizes = [ 32, 16, 2])\n",
    "\n",
    "file_path = \"parallel_cnn3_mitbih.h5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5, verbose=1)\n",
    "redonplat = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", patience=3, verbose=2)\n",
    "callbacks_list = [checkpoint, early, redonplat]  # early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "95e4b8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 78798 samples, validate on 8756 samples\n",
      "Epoch 1/1000\n",
      " - 126s - loss: 0.1770 - acc: 0.9519 - val_loss: 0.1405 - val_acc: 0.9613\n",
      "\n",
      "Epoch 00001: val_acc improved from 0.95066 to 0.96128, saving model to parallel_cnn3_mitbih.h5\n",
      "Epoch 2/1000\n",
      " - 125s - loss: 0.1540 - acc: 0.9582 - val_loss: 0.1213 - val_acc: 0.9672\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.96128 to 0.96722, saving model to parallel_cnn3_mitbih.h5\n",
      "Epoch 3/1000\n",
      " - 126s - loss: 0.1378 - acc: 0.9625 - val_loss: 0.1119 - val_acc: 0.9711\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.96722 to 0.97111, saving model to parallel_cnn3_mitbih.h5\n",
      "Epoch 4/1000\n",
      " - 128s - loss: 0.1264 - acc: 0.9661 - val_loss: 0.1065 - val_acc: 0.9717\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.97111 to 0.97168, saving model to parallel_cnn3_mitbih.h5\n",
      "Epoch 5/1000\n",
      " - 126s - loss: 0.1187 - acc: 0.9680 - val_loss: 0.1034 - val_acc: 0.9730\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.97168 to 0.97305, saving model to parallel_cnn3_mitbih.h5\n",
      "Epoch 6/1000\n",
      " - 126s - loss: 0.1122 - acc: 0.9696 - val_loss: 0.0954 - val_acc: 0.9748\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.97305 to 0.97476, saving model to parallel_cnn3_mitbih.h5\n",
      "Epoch 7/1000\n",
      " - 126s - loss: 0.1067 - acc: 0.9711 - val_loss: 0.1090 - val_acc: 0.9689\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.97476\n",
      "Epoch 8/1000\n",
      " - 126s - loss: 0.1022 - acc: 0.9718 - val_loss: 0.0915 - val_acc: 0.9773\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.97476 to 0.97727, saving model to parallel_cnn3_mitbih.h5\n",
      "Epoch 9/1000\n",
      " - 126s - loss: 0.0982 - acc: 0.9730 - val_loss: 0.0855 - val_acc: 0.9759\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.97727\n",
      "Epoch 10/1000\n",
      " - 126s - loss: 0.0978 - acc: 0.9734 - val_loss: 0.0897 - val_acc: 0.9759\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.97727\n",
      "Epoch 11/1000\n",
      " - 126s - loss: 0.0934 - acc: 0.9739 - val_loss: 0.0811 - val_acc: 0.9781\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.97727 to 0.97807, saving model to parallel_cnn3_mitbih.h5\n",
      "Epoch 12/1000\n",
      " - 125s - loss: 0.0914 - acc: 0.9751 - val_loss: 0.0821 - val_acc: 0.9786\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.97807 to 0.97864, saving model to parallel_cnn3_mitbih.h5\n",
      "Epoch 13/1000\n",
      " - 125s - loss: 0.0883 - acc: 0.9752 - val_loss: 0.0773 - val_acc: 0.9786\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.97864\n",
      "Epoch 14/1000\n",
      " - 125s - loss: 0.0863 - acc: 0.9763 - val_loss: 0.0760 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.97864 to 0.98001, saving model to parallel_cnn3_mitbih.h5\n",
      "Epoch 15/1000\n",
      " - 125s - loss: 0.0849 - acc: 0.9760 - val_loss: 0.0706 - val_acc: 0.9810\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.98001 to 0.98104, saving model to parallel_cnn3_mitbih.h5\n",
      "Epoch 16/1000\n",
      " - 124s - loss: 0.0829 - acc: 0.9764 - val_loss: 0.0674 - val_acc: 0.9806\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.98104\n",
      "Epoch 17/1000\n",
      " - 124s - loss: 0.0796 - acc: 0.9774 - val_loss: 0.0707 - val_acc: 0.9807\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.98104\n",
      "Epoch 18/1000\n",
      " - 125s - loss: 0.0785 - acc: 0.9783 - val_loss: 0.0692 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.98104\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 19/1000\n",
      " - 130s - loss: 0.0637 - acc: 0.9819 - val_loss: 0.0593 - val_acc: 0.9845\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.98104 to 0.98447, saving model to parallel_cnn3_mitbih.h5\n",
      "Epoch 20/1000\n",
      " - 130s - loss: 0.0591 - acc: 0.9826 - val_loss: 0.0579 - val_acc: 0.9839\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.98447\n",
      "Epoch 21/1000\n",
      " - 130s - loss: 0.0586 - acc: 0.9828 - val_loss: 0.0589 - val_acc: 0.9841\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.98447\n",
      "Epoch 22/1000\n",
      " - 130s - loss: 0.0566 - acc: 0.9833 - val_loss: 0.0567 - val_acc: 0.9850\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.98447 to 0.98504, saving model to parallel_cnn3_mitbih.h5\n",
      "Epoch 23/1000\n",
      " - 130s - loss: 0.0556 - acc: 0.9838 - val_loss: 0.0575 - val_acc: 0.9846\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.98504\n",
      "Epoch 24/1000\n",
      " - 130s - loss: 0.0551 - acc: 0.9835 - val_loss: 0.0569 - val_acc: 0.9856\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.98504 to 0.98561, saving model to parallel_cnn3_mitbih.h5\n",
      "Epoch 25/1000\n",
      " - 130s - loss: 0.0539 - acc: 0.9841 - val_loss: 0.0564 - val_acc: 0.9853\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.98561\n",
      "Epoch 26/1000\n",
      " - 131s - loss: 0.0538 - acc: 0.9842 - val_loss: 0.0563 - val_acc: 0.9850\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.98561\n",
      "Epoch 27/1000\n",
      " - 130s - loss: 0.0524 - acc: 0.9846 - val_loss: 0.0571 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.98561\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 28/1000\n",
      " - 130s - loss: 0.0515 - acc: 0.9844 - val_loss: 0.0565 - val_acc: 0.9849\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.98561\n",
      "Epoch 29/1000\n",
      " - 130s - loss: 0.0514 - acc: 0.9846 - val_loss: 0.0562 - val_acc: 0.9850\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.98561\n",
      "Epoch 00029: early stopping\n"
     ]
    }
   ],
   "source": [
    "X_train = [X, X_sample_small, X_sample_medium, X_sample_large]\n",
    "X_test_array = [X_test, X_test_sample_small, X_test_sample_medium, X_test_sample_large]\n",
    "\n",
    "#model.fit([X, X_smooth_small, X_smooth_medium, X_smooth_large], Y, epochs=1000, verbose=2, callbacks=callbacks_list, validation_split=0.1)\n",
    "model3.fit(X_train, Y, epochs=1000, verbose=2, callbacks=callbacks_list, validation_split=0.1)\n",
    "model3.load_weights(file_path)\n",
    "\n",
    "pred_test = model3.predict(X_test_array)\n",
    "pred_test = np.argmax(pred_test, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "770579d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test f1 score : 0.9038775701760254 \n",
      "Test accuracy score : 0.9835556367622876 \n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(Y_test, pred_test, average=\"macro\")\n",
    "\n",
    "print(\"Test f1 score : %s \"% f1)\n",
    "\n",
    "acc = accuracy_score(Y_test, pred_test)\n",
    "\n",
    "print(\"Test accuracy score : %s \"% acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9e00b31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model4(inputs_len = [187, 156 ,133 , 117], filter_sizes = [ 32, 16, 8]):\n",
    "    nclass = 5\n",
    "    \n",
    "    inp_smallseq = Input(shape=(inputs_len[1], 1))\n",
    "    \n",
    "    inp_smooth1 = Input(shape=(inputs_len[0], 1))   \n",
    "    inp_org = Input(shape=(inputs_len[0], 1))\n",
    "    \n",
    "    base_net_org = get_base_model(inputs_len[0], filter_sizes[0])\n",
    "    \n",
    "    #smoothing\n",
    "    base_net1 = get_base_model(inputs_len[0], filter_sizes[0])\n",
    "    \n",
    "    #sampling\n",
    "    base_net_small = get_base_model(inputs_len[1], filter_sizes[0])\n",
    "    \n",
    "    embedding_org = base_net_org(inp_org)\n",
    "    \n",
    "    #smoothing\n",
    "    embedding_smooth1 = base_net1(inp_smooth1)\n",
    "    \n",
    "    #sampling\n",
    "    embedding_small = base_net_small(inp_smallseq)\n",
    "    \n",
    "    # merge all the outputs\n",
    "    merged4 = Concatenate(axis = 1)([embedding_org,embedding_smooth1, embedding_small ]) \n",
    "\n",
    "    opt = optimizers.Adam(0.001) \n",
    "\n",
    "    merged4= Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"valid\")(merged4)\n",
    "    merged4 = GlobalMaxPool1D()(merged4)\n",
    "    merged4 = Dropout(rate=0.2)(merged4)\n",
    "    out4 = Dense(64, activation=activations.relu, name=\"dense_3_mitbih\")(merged4)\n",
    "    out4 = Dense(nclass, activation=activations.softmax, name=\"dense_4_mitbih\")(out4)   \n",
    "\n",
    "    model4 = models.Model(inputs=[inp_org,inp_smooth1,inp_smallseq], outputs=out4)\n",
    "    model4.compile(optimizer=opt, loss=losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
    "    model4.summary()  \n",
    "    \n",
    "    return model4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "dffbdeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_315 (InputLayer)          (None, 187, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_314 (InputLayer)          (None, 187, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_313 (InputLayer)          (None, 156, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_152 (Model)               (None, 20, 64)       12064       input_315[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "model_153 (Model)               (None, 20, 64)       12064       input_314[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "model_154 (Model)               (None, 16, 64)       12064       input_313[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 56, 64)       0           model_152[1][0]                  \n",
      "                                                                 model_153[1][0]                  \n",
      "                                                                 model_154[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_888 (Conv1D)             (None, 54, 256)      49408       concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_19 (Global (None, 256)          0           conv1d_888[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_583 (Dropout)           (None, 256)          0           global_max_pooling1d_19[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3_mitbih (Dense)          (None, 64)           16448       dropout_583[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_4_mitbih (Dense)          (None, 5)            325         dense_3_mitbih[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 102,373\n",
      "Trainable params: 102,373\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4 = get_model4(inputs_len = [187, 156 ,133 , 117], filter_sizes = [ 32, 16, 2])\n",
    "\n",
    "file_path = \"parallel_cnn4_mitbih.h5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5, verbose=1)\n",
    "redonplat = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", patience=3, verbose=2)\n",
    "callbacks_list = [checkpoint, early, redonplat]  # early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "745df19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 78798 samples, validate on 8756 samples\n",
      "Epoch 1/1000\n",
      " - 126s - loss: nan - acc: 0.8340 - val_loss: nan - val_acc: 0.8296\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.82960, saving model to parallel_cnn4_mitbih.h5\n",
      "Epoch 2/1000\n",
      " - 122s - loss: nan - acc: 0.8275 - val_loss: nan - val_acc: 0.8296\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.82960\n",
      "Epoch 3/1000\n",
      " - 123s - loss: nan - acc: 0.8275 - val_loss: nan - val_acc: 0.8296\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.82960\n",
      "Epoch 4/1000\n",
      " - 122s - loss: nan - acc: 0.8275 - val_loss: nan - val_acc: 0.8296\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.82960\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 5/1000\n",
      " - 133s - loss: nan - acc: 0.8275 - val_loss: nan - val_acc: 0.8296\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.82960\n",
      "Epoch 6/1000\n",
      " - 133s - loss: nan - acc: 0.8275 - val_loss: nan - val_acc: 0.8296\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.82960\n",
      "Epoch 00006: early stopping\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_245 to have shape (47, 1) but got array with shape (156, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14540\\4273941066.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mpred_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mpred_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\envs\\ml4h\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1147\u001b[0m                              'argument.')\n\u001b[0;32m   1148\u001b[0m         \u001b[1;31m# Validate user data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1149\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1150\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\envs\\ml4h\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\envs\\ml4h\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    136\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected input_245 to have shape (47, 1) but got array with shape (156, 1)"
     ]
    }
   ],
   "source": [
    "X_train = [X, X_smooth_small, X_sample_small]\n",
    "X_test_array = [X_test, X_test_smooth_small, X_test_sample_small]\n",
    "\n",
    "#model.fit([X, X_smooth_small, X_smooth_medium, X_smooth_large], Y, epochs=1000, verbose=2, callbacks=callbacks_list, validation_split=0.1)\n",
    "model4.fit(X_train, Y, epochs=1000, verbose=2, callbacks=callbacks_list, validation_split=0.1)\n",
    "model4.load_weights(file_path)\n",
    "\n",
    "pred_test = model4.predict(X_test_array)\n",
    "pred_test = np.argmax(pred_test, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "78609f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model4.predict(X_test_array)\n",
    "pred_test = np.argmax(pred_test, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "017aa13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test f1 score : 0.18113471632091976 \n",
      "Test accuracy score : 0.8276082587246483 \n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(Y_test, pred_test, average=\"macro\")\n",
    "\n",
    "print(\"Test f1 score : %s \"% f1)\n",
    "\n",
    "acc = accuracy_score(Y_test, pred_test)\n",
    "\n",
    "print(\"Test accuracy score : %s \"% acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5a4524",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
